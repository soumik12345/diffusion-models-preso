<!doctype html>
<html lang="en">
	<head>
		<meta charset="utf-8">
		<meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=no">

		<title>reveal.js</title>

		<link rel="stylesheet" href="dist/reset.css">
		<link rel="stylesheet" href="dist/reveal.css">
		<link rel="stylesheet" href="dist/theme/night.css">

		<!-- Theme used for syntax highlighted code -->
		<link rel="stylesheet" href="plugin/highlight/monokai.css">
		<style>
			.circular_image {
				width: 70%;
				height: 70%;
				border-radius: 50%;
				overflow: hidden;
				display:inline-block;
				vertical-align:middle;
			  }
			  .circular_image img{
				width:100%;
			  }

			  #left {
				left:-8.33%;
				text-align: left;
				float: left;
				width:50%;
				z-index:-10;
			  }
			  
			  #right {
				left:31.25%;
				top: 75px;
				float: right;
				text-align: right;
				z-index:-10;
				width:50%;
			  }
			  .container {
				display: flex;
			  }

			  .col {
				flex: 1;
			  }
		</style>
	</head>
	<body>
		<div class="reveal">
			<div class="slides">
				<!-- Slide 1 -->
				<section data-markdown>
					<textarea data-template>
					  ## A Landscape of [Diffusion Models]() in [2024]()

					  Presented by [Soumik Rakshit](https://geekyrakshit.dev/)
					</textarea>
				</section>
				<!-- Slide 2 -->
				<section data-markdown>
					<textarea data-template>
						<div id="left">
							<p>
								Follow along at <br>
								<p style="padding-top: 5%;">
									<a href="bit.ly/diffusion-2024">bit.ly/diffusion-2024</a>
								</p>
							</p>
						</div>
						<div id="right">
							<img src="./examples/assets/qr1.png" alt="">
						</div>
					</textarea>
				</section>
				<!-- Slide 3 -->
				<section data-markdown>
					<textarea data-template>
						<div id="left">
							<img class="circular_image" src="https://geekyrakshit.dev/assets/soumik_rakshit.png" alt="">
						</div>
						<div id="right">
							<p style="text-align: left; font-size: 80%; padding-bottom: 5%;">$>whoami</p>
							<p style="text-align: left; font-size: 50%; line-height: 175%;">
								👋 Building tools for ML using ML <a href="https://wandb.ai/site">@WandB</a>.<br>
								👋 Google Developer Expert in ML (JAX).<br>
								👋 Former ML Research @IBM and @Ignitarium.<br>
								👋 I build MLOps pipelines for open-source repositories (Keras, 🤗 Diffusers, MonAI, etc.)<br>
								👋 I am actively working on <a href="https://github.com/soumik12345/wandb-addons"></a>WandB-Addons</a>.<br>
								👋 Playing 🎻 between work; MineCraft and Elden Ring after work.<br>
								👋 More about myself at <a href="https://geekyrakshit.dev/">geekyrakshit.dev</a>
							</p>
						</div>
					</textarea>
				</section>
				<!-- Slide 4 -->
				<section>
					<section>
						Diffusion Models are a new family of generative models have exploded in popularity in the last few years.
					</section>
					<section>
						<img src="./examples/assets/landscape2.png" alt="">
					</section>
					<section>
						The idea was proposed by the paper
						<a href="https://arxiv.org/pdf/2006.11239.pdf">Denoising Diffusion Probabilistic Models</a>
						in 2021, and the world was never the same again.
					</section>
					<section>
						In this presentation, first we will explore some image generation techniques pre-dating diffusion models.
					</section>
					<section>
						<img src="./examples/assets/landscape.png" alt="">
					</section>
					<section>
						Next we will breifly explore the basic idea behind diffusion models.
					</section>
					<section>
						After that, we will look at a simple and flexible pipeline for training diffusion models for <a href="https://geekyrakshit.dev/diffusion-models-preso#/3/2">unconditional image generation</a>
					</section>
					<section>
						Finally, we will look at some of the recent advancements in diffusion models and their applications.
					</section>
				</section>
				<!-- Slide 5 -->
				<section data-markdown>
					<textarea data-template>
						## Stone Age of Image Generation
  
						[The era before the dawn of Diffusion]()
					  </textarea>
				</section>
				<!-- Slide 6 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							Let's talk about
							<a href="#/5"><strong>AutoEncoders</strong></a>
						</p>
					</section>
					<section>
						<img src="./examples/assets/autoencoder1.png">
						Let's say we want to transfer this image of a dog over a network.
					</section>
					<section>
						Long image data isn't compact and may take a long time to transfer over a network.
					</section>
					<section>
						So... how do we overcome this network bandwidth bottleneck?
					</section>
					<section>
						<img src="./examples/assets/autoencoder2.png">
						<a href="#/5/4">The Plan:</a>
						We can compress the image data at the source.
						Then send the compressed image over the network.
						Finally reconstruct the original image from the compressed one at the destination.
					</section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							A solid plan, right?
						</p>
					</section>
					<section>The question is how do we compress the image and then reconstruct it?</section>
					<section>
						How do we <a href="#5/7">teach a neural network</a> compress the image and then reconstruct it?
					</section>
					<section>
						We are surrounded by a lot of very high-dimensional data.
						\[\begin{aligned}
						x \in \mathbb{R}^{D}
						\end{aligned} \]
					</section>
					<section>
						<img src="./examples/assets/autoencoder6.png" alt="">
						<a href="#5/8">AutoEncoders</a> are a type of Unsupervised Neural Network that can learn a simple representation of the data.
					</section>
					<section>
						<img src="./examples/assets/autoencoder4.png" alt="">
					</section>
					<section>
						The objective of an Autoencoder is to learn a reconstruction loss $$ min Loss(x, r) $$ for a given set of high-dimensional data $$ x \in \mathbb{R}^{D} $$
					</section>
					<section>
						AutoEncoders are easy to train, but they don't always produce good results.
					</section>
					<section>
						Moreover, you can't generate new data from them.
					</section>
					<section>
						<div class="container">
							<div class="col">
								If you want to learn more about AutoEncoders, you can check out this excellent video 👉
							</div>
							<div class="col">
								<iframe width="560" height="315" src="https://www.youtube.com/embed/7mRfwaGGAPg?si=e068hmAARY2fgG5B" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
							</div>
						</div>
					</section>
				</section>
				<!-- Slide 7 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							Let's talk about
						</p>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/5"><strong>Generative Adversarial Networks</strong></a>
						</p>
					</section>
					<section>
						The simplest way to understand GANs is to think of them as a game between two neural networks: A <a href="#6/1">generator</a> competing against a <a href="#6/1">discriminator</a>.
					</section>
					<section>
						<img src="./examples/assets/gan1.png" alt="">
						Think of the generator as the <a href="#6/2">decoder of an AutoEncoder</a> that takes a latent vector sampled from a distribution and generates some new data.
					</section>
					<section>
						The <a href="#6/3">discriminator</a> is a binary classifier that tries to distinguish between real data belonging to the dataset and fake data generated by the generator.
					</section>
					<section style="text-align: left;">
						The approach sound cool on paper. But has a few problems...
					</section>
					<section>
						GANs are hard to train. They are very sensitive to hyperparameters and require a lot of tuning.
					</section>
					<section>
						The loss landscape of GANs are difficult to converge in practice.
					</section>
					<section>
						They often run into the problem of <a href="#6/7">Mode Collapse</a>.
					</section>
					<section>
						<div class="container">
							<div class="col">
								If you want to learn more about GANs, you can check out this excellent video 👉
							</div>
							<div class="col">
								<iframe width="560" height="315" src="https://www.youtube.com/embed/O8LAi6ksC80?si=5c6S86ouqnEv925e" title="YouTube video player" frameborder="0" allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share" allowfullscreen></iframe>
							</div>
						</div>
					</section>
				</section>
				<!-- Slide 8 -->
				<section data-markdown>
					<textarea data-template>
						## [Diffusion Models]()
					  </textarea>
				</section>
				<!-- Slide 9 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							What are
							<a href="#/5"><strong>Diffusion Models</strong></a>?
						</p>
					</section>
					<section>
						On the most basic level, diffusion models work by adding noise to training data and then denoising that data to recover it.
						<img src="./examples/assets/diffusion1.png" alt="">
					</section>
					<section>
						The <a href="#8/2">Forward Diffusion</a> process gradually adds noise to the image.
						<img src="./examples/assets/diffusion2.png" alt="">
					</section>
					<section>
						The <a href="#8/3">Reverse Diffusion</a> process learns to generate the image by gradually denoiseing.
						<img src="./examples/assets/diffusion3.png" alt="">
					</section>
					<section>
						Diffusion models are a family of latent variable based deep generative models that can synthesize diverse and high-quality images (and other forms of data as well, such as audio) that are  inspired by considerations from <a href="https://arxiv.org/abs/1503.03585">nonequilibrium thermodynamics</a>.
					</section>
					<section>
						These models have actually been around for a while... or at least their underlying principles have been. However, only recently have we been able to harness their potential for training deep generative models that can generate high-quality images while offering desirable properties such as <a href="#8/5">distribution coverage</a>, a <a href="#8/5">stationary training objective</a>, and <a href="#8/5">easy scalability</a>.
					</section>
				</section>
				<!-- Slide 10 -->
				<section data-markdown>
					<textarea data-template>
						## [Diffusion Models from a Thermodynamic Lense]()
					  </textarea>
				</section>
				<!-- Slide 11 -->
				<section>
					<section>
						The thermodynamic equilibrium of a thermodynamic system is the state or condition of the system when its properties do not change with time and that can be changed to another condition only at the expense of effects on other systems.
					</section>
					<section>
						Let's say that we have a can of air freshener. It has a lot of perfumed aerosol molecules under high pressure.
					</section>
					<section>
						Once you spray it in your room, initially, there is a high concentration of aerosol in the air.
					</section>
					<section>
						But the aerosol eventually spreads across the whole room from the point where it was sprayed to a lower concentration.
					</section>
					<section>
						This evens out its concentration across the room, achieving <a href="#10/4">thermodynamic equilibrium</a>.
					</section>
					<section>
						This process of transitioning into a state of thermodynamic equilibrium is called <a href="#10/5">diffusion</a>.
					</section>
					<section>
						<p>Similar to this thermodynamic phenomenon, a probabilistic diffusion model is basically a <a href="#10/6">Markov Chain</a> where noise is added to the data at each step of the chain. This process is referred to as the <a href="#10/6">Forward Diffusion</a> process, where we add a certain amount of noise to the data (an input image) sequentially.</p>
						<p>At the end of this Markov Chain, the input signal is completely destroyed, and we get an image that is purely noise.</p>
					</section>
					<section>
						A <a href="#10/7">Markov Chain</a> is a stochastic model describing a sequence of possible events in which the probability of each event depends only on the state attained in the previous event.
					</section>
					<section>
						<p>Now that the input signal has been reduced to noise after a finite number of steps, we can train a neural network to reverse this forward diffusion process.</p>
						<p>This <a href="#10/8">Reverse Diffusion</a> process involves the network being applied at each step to generate the image from the current time step to the previous one.</p>
					</section>
				</section>
				<!-- Slide 12 -->
				<section data-markdown>
					<textarea data-template>
						## [Let's train a Diffusion Model]()
						</textarea>
				</section>
				<!-- Slide 13 -->
				<section>
					<section data-markdown>
						<textarea data-template>
							### [Tools of the Trade]()
						  </textarea>
					</section>
					<section>
						<img src="./examples/assets/diffusers.png" alt="">
					</section>
					<section data-markdown>
						<textarea data-template>
							[Diffusers](https://huggingface.co/docs/diffusers/index) is a library built by [HuggingFace 🤗](https://huggingface.co/) that provides pre-trained diffusion models and serves as a modular toolbox for the training and inference of such models.
						</textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							### [🤗 Accelerate](https://huggingface.co/docs/accelerate/en/index)
						  </textarea>
					</section>
					<section data-markdown>
						<textarea data-template>
							[Accelerate](https://huggingface.co/docs/accelerate/v0.14.0/en/index) is another library built by [HuggingFace](https://huggingface.co/).
							Accelerate makes training and inference at scale made simple, efficient, and adaptable.
						  </textarea>
					</section>
					<section>
						<iframe src="https://wandb.ai/geekyrakshit/diffusers-image-generation/reports/Training-an-Unconditional-Image-Generation-Using-HuggingFace-Diffusers--Vmlldzo3MTU5Nzk0" style="border:none;height:1024px;width:100%"></iframe>
					</section>
				</section>
				<!-- Slide 14 -->
				<section data-markdown>
					<textarea data-template>
						## [Foundation Models for Image Generation]()
					  </textarea>
				</section>
				<!-- Slide 15 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/5"><strong>Classifier-free Guidance</strong></a>
						</p>
					</section>
					<section>
						In the traditional setup, some generative models, especially those used for tasks like conditional image generation or text-to-image synthesis, might use classifiers to guide the generation process towards the desired outcome. These classifiers can be used to determine how well the generated content matches the target criteria or instructions.
					</section>
					<section>
						However, classifier-free guidance eliminates the need for such external classifiers. Instead, the approach involves training the generative model in a way that it learns to modulate its outputs based on the given conditions or prompts directly, without relying on separate classifier feedback. 
					</section>
					<section>
						This is often achieved by training the model on a mix of conditioned (with specific prompts) and unconditioned (without prompts) data, and then adjusting the generation process based on the desired level of adherence to the prompt.
					</section>
					<section>
						<p>
							One of the key benefits of classifier-free guidance is that it simplifies the model architecture and training process by removing the need for additional classifier components.
						</p>
						<p>
							It can also potentially lead to more flexible and creative outputs, as the model learns to navigate the space of possible generations more freely.
						</p>
					</section>
				</section>
				<!-- Slide 16 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/5"><strong>Latent Diffusion</strong></a>
						</p>
					</section>
					<section>
						Latent diffusion models are a type of generative model that operates in a latent space, which is a compressed representation of data, instead of directly in the pixel space.
					</section>
					<section>
						The latent space is typically much smaller in dimensionality than the original data space, making the model more efficient and faster to train and generate samples from, as compared to traditional diffusion models that operate directly in the pixel or data space.
					</section>
				</section>
				<!-- Slide 17 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/16"><strong>Stable Diffusion</strong></a>
						</p>
					</section>
					<section>
						Stable Diffusion is powerful text-to-image generation model developed by <a href="https://stability.ai/">StabilityAI</a>.
					</section>
					<section>
						<img src="./examples/assets/sd1.jpeg" alt="">
					</section>
					<section>
						Stable Diffusion is a Latent Diffusion Model that is trained to generate high-quality images from text prompts.
					</section>
					<section>
						To condition the image generation on textual inputs, Stable Diffusion leverages embeddings from a model like CLIP (Contrastive Language–Image Pre-training), which understands and aligns the semantics of text and images. These embeddings guide the reverse diffusion process towards generating images that match the text description.
					</section>
					<section>
						The core of Stable Diffusion's architecture involves an autoencoder that compresses images into a lower-dimensional latent space and decompresses them back to image space. The autoencoder consists of an encoder that maps high-resolution images into a compact latent representation, and a decoder that reconstructs images from this latent space.
					</section>
					<section>
						By operating in a latent space, Stable Diffusion requires less computational power and memory than models that operate directly on pixels, making high-quality text-to-image generation more accessible.
					</section>
				</section>
				<!-- Slide 18 -->
				<section data-markdown>
					<textarea data-template>
						<div id="left">
							<p>
								Let's generate some images using Stable Diffusion <br>
								<p style="padding-top: 5%;">
									<a href="bit.ly/sd-notebook">bit.ly/sd-notebook</a>
								</p>
							</p>
						</div>
						<div id="right">
							<img src="./examples/assets/qr2.png" alt="">
						</div>
					</textarea>
				</section>
				<!-- Slide 19 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/16"><strong>Würstchen</strong></a>
						</p>
					</section>
					<section>
						Würstchen is another powerful text-to-image generation model developed by <a href="https://huggingface.co/warp-ai">WarpAI</a>.
					</section>
				</section>
				<!-- Slide 20 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/19"><strong>The Kandinsky Family</strong></a>
						</p>
					</section>
					<section>
						Würstchen is another powerful text-to-image generation model developed by <a href="https://huggingface.co/warp-ai">WarpAI</a>.
					</section>
				</section>
				<!-- Slide 21 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/19"><strong>Stable Diffusion XL</strong></a>
						</p>
					</section>
					<section>
						<iframe src="https://wandb.ai/geekyrakshit/stable-diffusion-xl/reports/A-Guide-to-Using-Stable-Diffusion-XL-with-HuggingFace-Diffusers-and-W-B--Vmlldzo1MDMxMDU4" style="border:none;height:1024px;width:100%"></iframe>
					</section>
				</section>
				<!-- Slide 22 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/19"><strong>PIXART-α</strong></a>
						</p>
					</section>
					<section>
						<iframe src="https://wandb.ai/geekyrakshit/pixart-alpha/reports/PIXART-A-Diffusion-Transformer-Model-for-Text-to-Image-Generation--Vmlldzo2MTE1NzM3" style="border:none;height:1024px;width:100%"></iframe>
					</section>
				</section>
				<!-- Slide 23 -->
				<section>
					<section>
						<p style="font-size: 150%; padding-bottom: 10%;">
							<a href="#/19"><strong>Prompt Engineering for Diffusion Models</strong></a>
						</p>
					</section>
					<section>
						<iframe src="https://wandb.ai/geekyrakshit/diffusers-prompt-engineering/reports/A-Guide-to-Prompt-Engineering-for-Stable-Diffusion--Vmlldzo1NzY4NzQ3" style="border:none;height:1024px;width:100%"></iframe>
					</section>
				</section>
				<!-- Slide 24 -->
				<section>
					<p style="font-size: 150%;"><a href="#/20">
						Thank You 🙏
					</a></p>
				</section>
			</div>
		</div>

		<script src="dist/reveal.js"></script>
		<script src="plugin/notes/notes.js"></script>
		<script src="plugin/markdown/markdown.js"></script>
		<script src="plugin/highlight/highlight.js"></script>
		<script src="plugin/math/math.js"></script>
		<script>
			// More info about initialization & config:
			// - https://revealjs.com/initialization/
			// - https://revealjs.com/config/
			Reveal.initialize({
				hash: true,

				// Learn about plugins: https://revealjs.com/plugins/
				plugins: [ RevealMarkdown, RevealHighlight, RevealNotes ]
			});
			Reveal.initialize({ plugins: [ RevealMath.KaTeX ] });
		</script>
	</body>
</html>
